[
  {
    "objectID": "kPCA_v_SVD.html",
    "href": "kPCA_v_SVD.html",
    "title": "Non-linear manifold learning",
    "section": "",
    "text": "This section focuses on using using the SVD to find a linear subspace of lower dimension in which to project the data. The objective is to show that dimensionality reduction works well in linear subspaces when the underlying structure is linear but quickly shows limitation for non-linear structures.\n\n\nWe first apply the SVD to an affine functions. The physical space of the dataset is \\(\\mathbb{R}^2\\) but the data set is in fact 1D. The SVD is used to find a suitable 1D linear subspace of \\(\\mathbb{R}^2\\) in which the data can be projected without loosing any information.\n\n\nWe start by defining the function and by computing the SVD of all the 2D snapshots.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nn_sample = 100                                                                              # Choose number of samples\nx = np.linspace(0,10,n_sample)                                                              # create vector x\n\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = a*x+b                                                                                   # Affine function\nX = np.stack((x,y))                                                                         # Create numpy array\n\n# Noise\nn_coef = 1                                                                                  # Noise magnitude\nX += + n_coef*np.random.rand(*X.shape)                                                      # Noisy data\n\n# SVD\nU,S,V = np.linalg.svd(X)                                                                   # SV Decomposition\n\n\n\n\n\nTo visualise the underlying space found in the data by the SVD, we plot the principal directions \\(\\nu_1\\) and \\(\\nu_2\\) onto the initial dataset.\n\n\nCode\ncolors_1D = plt.cm.viridis(np.linspace(0, 1, len(X[0,:])))                                        # Generate a color gradient\n\n# plots\n# Origin point\norigin = np.array([[0, 0], [0, 0]]) \nU_scaled = -5*U\nplt.figure(figsize=(6, 6))\n# plt.quiver(*origin, U_scaled[:, 0], U_scaled[:, 1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])\nplt.quiver(*origin, U_scaled[0, 0], U_scaled[0, 1], angles='xy', scale_units='xy', scale=1, color='#01426A', label=r'$\\nu_1$')\nplt.quiver(*origin, U_scaled[1, 0], U_scaled[1, 1], angles='xy', scale_units='xy', scale=1, color='#CE0037', label=r'$\\nu_2$')\nplt.legend()\nplt.gca().set_aspect('equal')\nplt.ylim(-4, 15)\nplt.xlim(-4, 15)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\n# plt.plot(X[0,:],X[1,:],'+')\n# plt.scatter(X[0,:],X[1,:], color=colors_1D, s=100)           \nplt.scatter(X[0,:],X[1,:], facecolors='none', edgecolors=colors_1D, s=100)             \n\nplt.title(\"Principal component base and data points\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nWe then project the data onto the latent space \\(\\left(\\nu_1,\\nu_2\\right)\\).\n\n\nCode\n# in the latent space\n\nX_tilde = np.transpose(U)@X\n# plt.plot(X_tilde[0,:],X_tilde[1,:],\"-o\")\nplt.scatter(X_tilde[0,:],X_tilde[1,:], color=colors_1D, s=100)           \n\nplt.gca().set_aspect('equal', adjustable='box')\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nWe can see that the initial 2D data in fact lies in a 1D space and that the first principal direction \\(\\nu_1\\) is sufficient to describe the full data set.\n\n\n\n\nTo show the limitation of the SVD to find non-linear manifold, a sin function is now studied in a similar manner.\n\n\nAgain, we start by defining the function and by computing the SVD of all the 2D snapshots.\n\n\nCode\n# In 1D for an sin function\n\n\nn_sample = 100                                                                              # Choose number of samples\n# x = np.linspace(0,np.pi/4,n_sample)\nx = np.linspace(0,6,n_sample)                                                               # create vector x\n\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = np.sin(x)                                                                               # Sin function\nX = np.stack((x,y))                                                                         # Create numpy array\n\n# Noise\nn_coef = 0.1                                                                                  # Noise magnitude\nX += + n_coef*np.random.rand(*X.shape)                                                      # Noisy data\n\n# SVD\nU,S,V = np.linalg.svd(X)                                                                   # SV Decomposition\n\n\n\n\n\nIn a similar manner, we plot the principal directions \\(\\nu_1\\) and \\(\\nu_2\\) onto the data set\n\n\nCode\n# plots\n\n\n# Origin point\norigin = np.array([[0, 0], [0, 0]]) \nU_scaled = -5*U\nplt.figure(figsize=(6, 6))\n# plt.quiver(*origin, U_scaled[:, 0], U_scaled[:, 1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])\nplt.quiver(*origin, U_scaled[0, 0], U_scaled[0, 1], angles='xy', scale_units='xy', scale=1, color='#01426A', label=r'$\\nu_1$')\nplt.quiver(*origin, U_scaled[1, 0], U_scaled[1, 1], angles='xy', scale_units='xy', scale=1, color='#CE0037', label=r'$\\nu_2$')\nplt.legend()\nplt.gca().set_aspect('equal')\n# plt.ylim(-4, 15)\n# plt.xlim(-4, 15)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\n# plt.plot(X[0,:],X[1,:],'+')\nplt.scatter(X[0,:],X[1,:], facecolors='none', edgecolors=colors_1D, s=100)             \n\nplt.title(\"Principal component base and data points - sin\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nAnd we project the data onto the latent space \\(\\left(\\nu_1,\\nu_2\\right)\\).\n\n\nCode\n# in the latent space\n\nX_tilde = np.transpose(U)@X\n# plt.plot(X_tilde[0,:],X_tilde[1,:],\"-o\")\nplt.scatter(X_tilde[0,:],X_tilde[1,:], color=colors_1D, s=100)           \n\nplt.gca().set_aspect('equal', adjustable='box')\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nThis time, the SVD does not exhibit a lower dimension space in which the data can be represented. Both principal directions are required to represent the non-linear data set."
  },
  {
    "objectID": "kPCA_v_SVD.html#kernel-principal-component-analysis-kpca",
    "href": "kPCA_v_SVD.html#kernel-principal-component-analysis-kpca",
    "title": "Non-linear manifold learning",
    "section": "",
    "text": "This section focuses on using using the kPCA to find a non-linear manifold of lower dimension in which to project the data. The objective is to show that dimensionality reduction works well in non-linear cases where the SVD failed.\n\n\nThe first application is an sin functions in R3\n\n\n\n\nCode\nx = np.linspace(0,4*np.pi,1500)\ny = np.sin(x)                                                                   # Sin function\nn_coef = 0                                                                      # Noise level\nz = np.random.randn(*x.shape)        \nX = np.stack((x,y,z))                                                           # Create 3D dataset\nX += + n_coef*np.random.rand(*X.shape)                                          # Add noise\n\n\nPCA conventions are transposed compared to SVD\n\n\nCode\nY = np.transpose(X)\n\n\n\n\n\n\nCode\nn_components = 2                                                                 # Number of component of the kpca\nsigma =100                                                                       # sigma for Gaussian kernel\n\n\n\n\n\n\n\nCode\nd_2 = np.sum(Y**2, axis=1).reshape(-1, 1) + np.sum(Y**2, axis=1) - 2 * np.dot(Y, Y.T)\nK = np.exp(-d_2 / (2 * sigma**2))\n\n\n\n\n\n\n\nCode\nn = K.shape[0]\none_n = np.ones((n, n)) / n\nK_centered = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n\n\nSVD of the centered kernel\n\n\nCode\nU, S, V = np.linalg.svd(K_centered)\n\n\nTruncation and projection\n\n\nCode\nU_k = U[:, :n_components]  # first eigenvectors\nS_k = np.diag(S[:n_components])  # first singular values\n# Project the centered kernel matrix onto the first eigenvectors\nY_kpca = U_k @ S_k  \n\n\n\n\n\n\n\nIn the physical space\n\n\nCode\ncolors = plt.cm.viridis(np.linspace(0, 1, len(X[0,:])))                                        # Generate a color gradient\n\nimport plotly.graph_objects as go\n\n\n# colors_plotly = colors[:, :3]\ncolors_plotly = np.linspace(0, 1, X.shape[1])\n\nx_points = X[0, :]\ny_points = X[1, :]\nz_points = X[2, :]\n\n# Create the 3D scatter plot\nfig = go.Figure(data=[go.Scatter3d(\n    x=x_points,\n    y=y_points,\n    z=z_points,\n    mode='markers',\n    marker=dict(\n        size=5,                                                                         # Size of the markers\n        color=colors_plotly,                                                            # Color of the markers\n        colorscale='viridis',                                                           # Color scale\n        showscale=True,                                                                 # Show color scale\n    )\n)])\n\nfig.update_layout(\n    scene=dict(\n        xaxis_title='x',\n        yaxis_title='y',\n        zaxis_title='z',\n    ),\n    title='Physical Space'\n)\nfig.show()\n\n\n                                                \n\n\nIn the latent space\n\n\nCode\nplt.scatter(Y_kpca[:,0], Y_kpca[:,1], color=colors, s=100)                                     # s=100 for larger points\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.title('latent space')\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A kPCA illustration",
    "section": "",
    "text": "Welcome\nTo this small illustration of non-linear manifold learning using kPCA"
  },
  {
    "objectID": "kPCA_v_SVD.html#svd",
    "href": "kPCA_v_SVD.html#svd",
    "title": "Non-linear manifold learning",
    "section": "",
    "text": "This section focuses on using using the SVD to find a linear subspace of lower dimension in which to project the data. The objective is to show that dimensionality reduction works well in linear subspaces when the underlying structure is linear but quickly shows limitation for non-linear structures.\n\n\nThe first application is an affine functions in R2\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nn_sample = 100                                                                              # Choose number of samples\nx = np.linspace(0,10,n_sample)                                                              # create vector x\n\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = a*x+b                                                                                   # Affine function\nX = np.stack((x,y))                                                                         # Create numpy array\n\n# Noise\nn_coef = 1                                                                                  # Noise magnitude\nX += + n_coef*np.random.rand(*X.shape)                                                      # Noisy data\n\n# SVD\nU,S,V = np.linalg.svd(X)                                                                   # SV Decomposition\n\n\n\n\n\nPlot the principal directions from onto the data set\n\n\nCode\n# plots\n# Origin point\norigin = np.array([[0, 0], [0, 0]]) \nU_scaled = -5*U\nplt.figure(figsize=(6, 6))\n# plt.quiver(*origin, U_scaled[:, 0], U_scaled[:, 1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])\nplt.quiver(*origin, U_scaled[0, 0], U_scaled[0, 1], angles='xy', scale_units='xy', scale=1, color=['r'], label=r'$\\nu_1$')\nplt.quiver(*origin, U_scaled[1, 0], U_scaled[1, 1], angles='xy', scale_units='xy', scale=1, color=['b'], label=r'$\\nu_2$')\nplt.legend()\nplt.gca().set_aspect('equal')\nplt.ylim(-4, 15)\nplt.xlim(-4, 15)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\nplt.plot(X[0,:],X[1,:],'+')\nplt.title(\"Principal component base and data points\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nAnd in the latent space\n\n\nCode\n# in the latent space\n\nX_tilde = np.transpose(U)@X\nplt.plot(X_tilde[0,:],X_tilde[1,:],\"-o\")\nplt.gca().set_aspect('equal', adjustable='box')\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nWe can see that the initial 2D data lie in a 1D space and that the first principal direction \\(\\nu_1\\) is sufficient to describe the data set.\n\n\n\n\nTo show the limitation of the SVD to find non-linear manifold, a sin function is now studied in a similar manner.\n\n\n\n\nCode\n# In 1D for an sin function\n\n\nn_sample = 100                                                                              # Choose number of samples\n# x = np.linspace(0,np.pi/4,n_sample)\nx = np.linspace(0,6,n_sample)                                                               # create vector x\n\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = np.sin(x)                                                                               # Sin function\nX = np.stack((x,y))                                                                         # Create numpy array\n\n# Noise\nn_coef = 0.1                                                                                  # Noise magnitude\nX += + n_coef*np.random.rand(*X.shape)                                                      # Noisy data\n\n# SVD\nU,S,V = np.linalg.svd(X)                                                                   # SV Decomposition\n\n\n\n\n\nPlot the principal directions from onto the data set\n\n\nCode\n# plots\n\n\n# Origin point\norigin = np.array([[0, 0], [0, 0]]) \nU_scaled = -5*U\nplt.figure(figsize=(6, 6))\n# plt.quiver(*origin, U_scaled[:, 0], U_scaled[:, 1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])\nplt.quiver(*origin, U_scaled[0, 0], U_scaled[0, 1], angles='xy', scale_units='xy', scale=1, color=['r'], label=r'$\\nu_1$')\nplt.quiver(*origin, U_scaled[1, 0], U_scaled[1, 1], angles='xy', scale_units='xy', scale=1, color=['b'], label=r'$\\nu_2$')\nplt.legend()\nplt.gca().set_aspect('equal')\n# plt.ylim(-4, 15)\n# plt.xlim(-4, 15)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\nplt.plot(X[0,:],X[1,:],'+')\n\nplt.title(\"Principal component base and data points - sin\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nAnd in the latent space\n\n\nCode\n# in the latent space\n\nX_tilde = np.transpose(U)@X\nplt.plot(X_tilde[0,:],X_tilde[1,:],\"-o\")\nplt.gca().set_aspect('equal', adjustable='box')\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nThe SVD does not exhibit a lower dimension space in which the data can be represented. Both principal directions are required to represent the non-linear data set."
  },
  {
    "objectID": "kPCA_v_SVD.html#application-to-affine-function",
    "href": "kPCA_v_SVD.html#application-to-affine-function",
    "title": "Non-linear manifold learning",
    "section": "",
    "text": "We first apply the SVD to an affine functions. The physical space of the dataset is \\(\\mathbb{R}^2\\) but the data set is in fact 1D. The SVD is used to find a suitable 1D linear subspace of \\(\\mathbb{R}^2\\) in which the data can be projected without loosing any information.\n\n\nWe start by defining the function and by computing the SVD of all the 2D snapshots.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nn_sample = 100                                                                              # Choose number of samples\nx = np.linspace(0,10,n_sample)                                                              # create vector x\n\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = a*x+b                                                                                   # Affine function\nX = np.stack((x,y))                                                                         # Create numpy array\n\n# Noise\nn_coef = 1                                                                                  # Noise magnitude\nX += + n_coef*np.random.rand(*X.shape)                                                      # Noisy data\n\n# SVD\nU,S,V = np.linalg.svd(X)                                                                   # SV Decomposition\n\n\n\n\n\nTo visualise the underlying space found in the data by the SVD, we plot the principal directions \\(\\nu_1\\) and \\(\\nu_2\\) onto the initial dataset.\n\n\nCode\ncolors_1D = plt.cm.viridis(np.linspace(0, 1, len(X[0,:])))                                        # Generate a color gradient\n\n# plots\n# Origin point\norigin = np.array([[0, 0], [0, 0]]) \nU_scaled = -5*U\nplt.figure(figsize=(6, 6))\n# plt.quiver(*origin, U_scaled[:, 0], U_scaled[:, 1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])\nplt.quiver(*origin, U_scaled[0, 0], U_scaled[0, 1], angles='xy', scale_units='xy', scale=1, color='#01426A', label=r'$\\nu_1$')\nplt.quiver(*origin, U_scaled[1, 0], U_scaled[1, 1], angles='xy', scale_units='xy', scale=1, color='#CE0037', label=r'$\\nu_2$')\nplt.legend()\nplt.gca().set_aspect('equal')\nplt.ylim(-4, 15)\nplt.xlim(-4, 15)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\n# plt.plot(X[0,:],X[1,:],'+')\n# plt.scatter(X[0,:],X[1,:], color=colors_1D, s=100)           \nplt.scatter(X[0,:],X[1,:], facecolors='none', edgecolors=colors_1D, s=100)             \n\nplt.title(\"Principal component base and data points\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nWe then project the data onto the latent space \\(\\left(\\nu_1,\\nu_2\\right)\\).\n\n\nCode\n# in the latent space\n\nX_tilde = np.transpose(U)@X\n# plt.plot(X_tilde[0,:],X_tilde[1,:],\"-o\")\nplt.scatter(X_tilde[0,:],X_tilde[1,:], color=colors_1D, s=100)           \n\nplt.gca().set_aspect('equal', adjustable='box')\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nWe can see that the initial 2D data in fact lies in a 1D space and that the first principal direction \\(\\nu_1\\) is sufficient to describe the full data set."
  },
  {
    "objectID": "kPCA_v_SVD.html#application-to-sin-function",
    "href": "kPCA_v_SVD.html#application-to-sin-function",
    "title": "Non-linear manifold learning",
    "section": "Application to sin function",
    "text": "Application to sin function\nWe first apply the SVD to an sin function. The physical space of the dataset is \\(\\mathbb{R}32\\) but the data set is in fact 2D. The kPCA is used to find a suitable 2D (non-linear) manifold in which the data can be projected without loosing any information.\n\nFunction definition\n\n\nCode\nx = np.linspace(0,4*np.pi,1500)\ny = np.sin(x)                                                                   # Sin function\nn_coef = 0                                                                      # Noise level\nz = np.random.randn(*x.shape)        \nX = np.stack((x,y,z))                                                           # Create 3D dataset\nX += + n_coef*np.random.rand(*X.shape)                                          # Add noise\n\n\nNote: The PCA conventions are transposed compared to SVD’s conventions\n\n\nCode\nY = np.transpose(X)\n\n\n\n\nkPCA parameters\nWe define the kPCA paramerets, namely\n\nThe number of components\n\\(\\sigma\\), the width of the kernel, controlling how much distant point are considered similar\n\nA smaller \\(\\sigma\\) emphasises local structures, while a larger σσ captures more global patterns.\n\n\n\n\nCode\nn_components = 2                                                                 # Number of component of the kpca\nsigma =100                                                                       # sigma for Gaussian kernel\n\n\n\n\nCompute the (Gaussian) kernel\nWe now compute the kernel matrix based on the distance matrix using a Gaussian kernel\n\\[K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\\]\n\n\nCode\nd_2 = np.sum(Y**2, axis=1).reshape(-1, 1) + np.sum(Y**2, axis=1) - 2 * np.dot(Y, Y.T)\nK = np.exp(-d_2 / (2 * sigma**2))\n\n\n\n\nCenter the kernel matrix\nWe then center the kernel matrix as the PCA expects 0-mean data\n\\[\nK_{\\text{centered}} = K - \\frac{1}{n} \\mathbf{1} K - \\frac{1}{n} K \\mathbf{1} + \\frac{1}{n^2} \\mathbf{1} K \\mathbf{1}\n\\]\n\n\nCode\nn = K.shape[0]\none_n = np.ones((n, n)) / n\nK_centered = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n\n\nWe perform an SVD of the centered kernel to find the principal directions of the high dimensional space,\n\n\nCode\nU, S, V = np.linalg.svd(K_centered)\n\n\nthat we then truncates. we then project the initial dataset onto the principal directions in the latent space\n\n\nCode\nU_k = U[:, :n_components]  # first eigenvectors\nS_k = np.diag(S[:n_components])  # first singular values\n# Project the centered kernel matrix onto the first eigenvectors\nY_kpca = U_k @ S_k  \n\n\n\n\nPlots\nIn the physical space (the plot is iteractive, feel free to explore the 3D dataset !)\n\n\nCode\ncolors = plt.cm.viridis(np.linspace(0, 1, len(X[0,:])))                                        # Generate a color gradient\n\nimport plotly.graph_objects as go\n\n\n# colors_plotly = colors[:, :3]\ncolors_plotly = np.linspace(0, 1, X.shape[1])\n\nx_points = X[0, :]\ny_points = X[1, :]\nz_points = X[2, :]\n\n# Create the 3D scatter plot\nfig = go.Figure(data=[go.Scatter3d(\n    x=x_points,\n    y=y_points,\n    z=z_points,\n    mode='markers',\n    marker=dict(\n        size=5,                                                                         # Size of the markers\n        color=colors_plotly,                                                            # Color of the markers\n        colorscale='viridis',                                                           # Color scale\n        showscale=True,                                                                 # Show color scale\n    )\n)])\n\nfig.update_layout(\n    scene=dict(\n        xaxis_title='x',\n        yaxis_title='y',\n        zaxis_title='z',\n    ),\n    title='Physical Space'\n)\nfig.show()\n\n\n                                                \n\n\nIn the latent space\n\n\nCode\nplt.scatter(Y_kpca[:,0], Y_kpca[:,1], color=colors, s=100)                                     # s=100 for larger points\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.title('latent space')\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that contrary to the SVD, the kPCA could decrease the dimensionnality of the supposidly 3D space into a 2D space, finding the underlying non-linear manifold onto which the dataset lies."
  },
  {
    "objectID": "kPCA_v_SVD.html#application-to-non-linear-function",
    "href": "kPCA_v_SVD.html#application-to-non-linear-function",
    "title": "Non-linear manifold learning",
    "section": "",
    "text": "To show the limitation of the SVD to find non-linear manifold, a sin function is now studied in a similar manner.\n\n\nAgain, we start by defining the function and by computing the SVD of all the 2D snapshots.\n\n\nCode\n# In 1D for an sin function\n\n\nn_sample = 100                                                                              # Choose number of samples\n# x = np.linspace(0,np.pi/4,n_sample)\nx = np.linspace(0,6,n_sample)                                                               # create vector x\n\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = np.sin(x)                                                                               # Sin function\nX = np.stack((x,y))                                                                         # Create numpy array\n\n# Noise\nn_coef = 0.1                                                                                  # Noise magnitude\nX += + n_coef*np.random.rand(*X.shape)                                                      # Noisy data\n\n# SVD\nU,S,V = np.linalg.svd(X)                                                                   # SV Decomposition\n\n\n\n\n\nIn a similar manner, we plot the principal directions \\(\\nu_1\\) and \\(\\nu_2\\) onto the data set\n\n\nCode\n# plots\n\n\n# Origin point\norigin = np.array([[0, 0], [0, 0]]) \nU_scaled = -5*U\nplt.figure(figsize=(6, 6))\n# plt.quiver(*origin, U_scaled[:, 0], U_scaled[:, 1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])\nplt.quiver(*origin, U_scaled[0, 0], U_scaled[0, 1], angles='xy', scale_units='xy', scale=1, color='#01426A', label=r'$\\nu_1$')\nplt.quiver(*origin, U_scaled[1, 0], U_scaled[1, 1], angles='xy', scale_units='xy', scale=1, color='#CE0037', label=r'$\\nu_2$')\nplt.legend()\nplt.gca().set_aspect('equal')\n# plt.ylim(-4, 15)\n# plt.xlim(-4, 15)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\n# plt.plot(X[0,:],X[1,:],'+')\nplt.scatter(X[0,:],X[1,:], facecolors='none', edgecolors=colors_1D, s=100)             \n\nplt.title(\"Principal component base and data points - sin\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nAnd we project the data onto the latent space \\(\\left(\\nu_1,\\nu_2\\right)\\).\n\n\nCode\n# in the latent space\n\nX_tilde = np.transpose(U)@X\n# plt.plot(X_tilde[0,:],X_tilde[1,:],\"-o\")\nplt.scatter(X_tilde[0,:],X_tilde[1,:], color=colors_1D, s=100)           \n\nplt.gca().set_aspect('equal', adjustable='box')\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nThis time, the SVD does not exhibit a lower dimension space in which the data can be represented. Both principal directions are required to represent the non-linear data set."
  },
  {
    "objectID": "kPCA_v_SVD.html#application-to-an-affine-function",
    "href": "kPCA_v_SVD.html#application-to-an-affine-function",
    "title": "Non-linear manifold learning",
    "section": "",
    "text": "We first apply the SVD to an affine functions. The physical space of the dataset is \\(\\mathbb{R}^2\\) but the data set is in fact 1D. The SVD is used to find a suitable 1D linear subspace of \\(\\mathbb{R}^2\\) in which the data can be projected without loosing any information.\n\n\nWe start by defining the function and by computing the SVD of all the 2D snapshots.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nn_sample = 100                                                                              # Choose number of samples\nx = np.linspace(0,10,n_sample)                                                              # create vector x\n\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = a*x+b                                                                                   # Affine function\nX = np.stack((x,y))                                                                         # Create numpy array\n\n# Noise\nn_coef = 1                                                                                  # Noise magnitude\nX += + n_coef*np.random.rand(*X.shape)                                                      # Noisy data\n\n# SVD\nU,S,V = np.linalg.svd(X)                                                                   # SV Decomposition\n\n\n\n\n\nTo visualise the underlying space found in the data by the SVD, we plot the principal directions \\(\\nu_1\\) and \\(\\nu_2\\) onto the initial dataset.\n\n\nCode\ncolors_1D = plt.cm.viridis(np.linspace(0, 1, len(X[0,:])))                                        # Generate a color gradient\n\n# plots\n# Origin point\norigin = np.array([[0, 0], [0, 0]]) \nU_scaled = -5*U\nplt.figure(figsize=(6, 6))\n# plt.quiver(*origin, U_scaled[:, 0], U_scaled[:, 1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])\nplt.quiver(*origin, U_scaled[0, 0], U_scaled[0, 1], angles='xy', scale_units='xy', scale=1, color='#01426A', label=r'$\\nu_1$')\nplt.quiver(*origin, U_scaled[1, 0], U_scaled[1, 1], angles='xy', scale_units='xy', scale=1, color='#CE0037', label=r'$\\nu_2$')\nplt.legend()\nplt.gca().set_aspect('equal')\nplt.ylim(-4, 15)\nplt.xlim(-4, 15)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\n# plt.plot(X[0,:],X[1,:],'+')\n# plt.scatter(X[0,:],X[1,:], color=colors_1D, s=100)           \nplt.scatter(X[0,:],X[1,:], facecolors='none', edgecolors=colors_1D, s=100)             \n\nplt.title(\"Principal component base and data points\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nWe then project the data onto the latent space \\(\\left(\\nu_1,\\nu_2\\right)\\).\n\n\nCode\n# in the latent space\n\nX_tilde = np.transpose(U)@X\n# plt.plot(X_tilde[0,:],X_tilde[1,:],\"-o\")\nplt.scatter(X_tilde[0,:],X_tilde[1,:], color=colors_1D, s=100)           \n\nplt.gca().set_aspect('equal', adjustable='box')\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nWe can see that the initial 2D data in fact lies in a 1D space and that the first principal direction \\(\\nu_1\\) is sufficient to describe the full data set."
  },
  {
    "objectID": "kPCA_v_SVD.html#application-to-a-sin-function",
    "href": "kPCA_v_SVD.html#application-to-a-sin-function",
    "title": "Non-linear manifold learning",
    "section": "Application to a sin function",
    "text": "Application to a sin function\nWe first apply the SVD to a sin function. The physical space of the dataset is \\(\\mathbb{R}3\\) but the data set is in fact 2D. The kPCA is used to find a suitable 2D (non-linear) manifold in which the data can be projected without loosing any information.\n\nFunction definition\n\n\nCode\nx = np.linspace(0,4*np.pi,1500)\ny = np.sin(x)                                                                   # Sin function\nn_coef = 0                                                                      # Noise level\nz = np.random.randn(*x.shape)        \nX = np.stack((x,y,z))                                                           # Create 3D dataset\nX += + n_coef*np.random.rand(*X.shape)                                          # Add noise\n\n\nNote: The PCA conventions are transposed compared to SVD’s conventions\n\n\nCode\nY = np.transpose(X)\n\n\n\n\nkPCA parameters\nWe define the kPCA paramerets, namely\n\nThe number of components\n\\(\\sigma\\), the width of the kernel, controlling how much distant point are considered similar\n\nA smaller \\(\\sigma\\) emphasises local structures, while a larger σσ captures more global patterns.\n\n\n\n\nCode\nn_components = 2                                                                 # Number of component of the kpca\nsigma =100                                                                       # sigma for Gaussian kernel\n\n\n\n\nCompute the (Gaussian) kernel\nWe now compute the kernel matrix based on the distance matrix using a Gaussian kernel\n\\[K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\\]\n\n\nCode\nd_2 = np.sum(Y**2, axis=1).reshape(-1, 1) + np.sum(Y**2, axis=1) - 2 * np.dot(Y, Y.T)\nK = np.exp(-d_2 / (2 * sigma**2))\n\n\n\n\nCenter the kernel matrix\nWe then center the kernel matrix as the PCA expects 0-mean data\n\\[\nK_{\\text{centered}} = K - \\frac{1}{n} \\mathbf{1} K - \\frac{1}{n} K \\mathbf{1} + \\frac{1}{n^2} \\mathbf{1} K \\mathbf{1}\n\\]\n\n\nCode\nn = K.shape[0]\none_n = np.ones((n, n)) / n\nK_centered = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n\n\nWe perform an SVD of the centered kernel to find the principal directions of the high dimensional space,\n\n\nCode\nU, S, V = np.linalg.svd(K_centered)\n\n\nthat we then truncates. we then project the initial dataset onto the principal directions in the latent space\n\n\nCode\nU_k = U[:, :n_components]  # first eigenvectors\nS_k = np.diag(S[:n_components])  # first singular values\n# Project the centered kernel matrix onto the first eigenvectors\nY_kpca = U_k @ S_k  \n\n\n\n\nPlots\nIn the physical space (the plot is interactive, feel free to explore the 3D dataset !)\n\n\nCode\ncolors = plt.cm.viridis(np.linspace(0, 1, len(X[0,:])))                                        # Generate a color gradient\n\nimport plotly.graph_objects as go\n\n\n# colors_plotly = colors[:, :3]\ncolors_plotly = np.linspace(0, 1, X.shape[1])\n\nx_points = X[0, :]\ny_points = X[1, :]\nz_points = X[2, :]\n\n# Create the 3D scatter plot\nfig = go.Figure(data=[go.Scatter3d(\n    x=x_points,\n    y=y_points,\n    z=z_points,\n    mode='markers',\n    marker=dict(\n        size=5,                                                                         # Size of the markers\n        color=colors_plotly,                                                            # Color of the markers\n        colorscale='viridis',                                                           # Color scale\n        showscale=True,                                                                 # Show color scale\n    )\n)])\n\nfig.update_layout(\n    scene=dict(\n        xaxis_title='x',\n        yaxis_title='y',\n        zaxis_title='z',\n    ),\n    title='Physical Space'\n)\nfig.show()\n\n\n                                                \n\n\nIn the latent space\n\n\nCode\nplt.scatter(Y_kpca[:,0], Y_kpca[:,1], color=colors, s=100)                                     # s=100 for larger points\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.title('latent space')\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that contrary to the SVD, the kPCA could decrease the dimensionnality of the supposidly 3D space into a 2D space, finding the underlying non-linear manifold onto which the dataset lies."
  },
  {
    "objectID": "kPCA_v_SVD.html#application-to-an-affine-function-1",
    "href": "kPCA_v_SVD.html#application-to-an-affine-function-1",
    "title": "Non-linear manifold learning",
    "section": "Application to an affine function",
    "text": "Application to an affine function\nWe first apply the SVD to an sin function. The physical space of the dataset is \\(\\mathbb{R}32\\) but the data set is in fact 2D. The kPCA is used to find a suitable 2D (non-linear) manifold in which the data can be projected without loosing any information.\n\nFunction definition\n\n\nCode\nx = np.linspace(0,4*np.pi,1500)\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = a*x+b \nn_coef = 1                                                                      # Noise level\nz = np.random.randn(*x.shape)        \nX = np.stack((x,y,z))                                                           # Create 3D dataset\nX += + n_coef*np.random.rand(*X.shape)                                          # Add noise\n\n\nNote: The PCA conventions are transposed compared to SVD’s conventions\n\n\nCode\nY = np.transpose(X)\n\n\n\n\nkPCA parameters\nWe define the kPCA paramerets, namely\n\nThe number of components\n\\(\\sigma\\), the width of the kernel, controlling how much distant point are considered similar\n\nA smaller \\(\\sigma\\) emphasises local structures, while a larger σσ captures more global patterns.\n\n\n\n\nCode\nn_components = 2                                                                 # Number of component of the kpca\nsigma =100                                                                       # sigma for Gaussian kernel\n\n\n\n\nCompute the (Gaussian) kernel\nWe now compute the kernel matrix based on the distance matrix using a Gaussian kernel\n\\[K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\\]\n\n\nCode\nd_2 = np.sum(Y**2, axis=1).reshape(-1, 1) + np.sum(Y**2, axis=1) - 2 * np.dot(Y, Y.T)\nK = np.exp(-d_2 / (2 * sigma**2))\n\n\n\n\nCenter the kernel matrix\nWe then center the kernel matrix as the PCA expects 0-mean data\n\\[\nK_{\\text{centered}} = K - \\frac{1}{n} \\mathbf{1} K - \\frac{1}{n} K \\mathbf{1} + \\frac{1}{n^2} \\mathbf{1} K \\mathbf{1}\n\\]\n\n\nCode\nn = K.shape[0]\none_n = np.ones((n, n)) / n\nK_centered = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n\n\nWe perform an SVD of the centered kernel to find the principal directions of the high dimensional space,\n\n\nCode\nU, S, V = np.linalg.svd(K_centered)\n\n\nthat we then truncates. we then project the initial dataset onto the principal directions in the latent space\n\n\nCode\nU_k = U[:, :n_components]  # first eigenvectors\nS_k = np.diag(S[:n_components])  # first singular values\n# Project the centered kernel matrix onto the first eigenvectors\nY_kpca = U_k @ S_k  \n\n\n\n\nPlots\nIn the physical space (the plot is iteractive, feel free to explore the 3D dataset !)\n\n\nCode\ncolors = plt.cm.viridis(np.linspace(0, 1, len(X[0,:])))                                        # Generate a color gradient\n\nimport plotly.graph_objects as go\n\n\n# colors_plotly = colors[:, :3]\ncolors_plotly = np.linspace(0, 1, X.shape[1])\n\nx_points = X[0, :]\ny_points = X[1, :]\nz_points = X[2, :]\n\n# Create the 3D scatter plot\nfig = go.Figure(data=[go.Scatter3d(\n    x=x_points,\n    y=y_points,\n    z=z_points,\n    mode='markers',\n    marker=dict(\n        size=5,                                                                         # Size of the markers\n        color=colors_plotly,                                                            # Color of the markers\n        colorscale='viridis',                                                           # Color scale\n        showscale=True,                                                                 # Show color scale\n    )\n)])\n\nfig.update_layout(\n    scene=dict(\n        xaxis_title='x',\n        yaxis_title='y',\n        zaxis_title='z',\n    ),\n    title='Physical Space'\n)\nfig.show()\n\n\n                                                \n\n\nIn the latent space\n\n\nCode\nplt.scatter(Y_kpca[:,0], Y_kpca[:,1], color=colors, s=100)                                     # s=100 for larger points\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.title('latent space')\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that contrary to the SVD, the kPCA could decrease the dimensionnality of the supposidly 3D space into a 2D space, finding the underlying non-linear manifold onto which the dataset lies."
  },
  {
    "objectID": "kPCA_v_SVD.html#application-to-a-noisy-affine-function",
    "href": "kPCA_v_SVD.html#application-to-a-noisy-affine-function",
    "title": "Non-linear manifold learning",
    "section": "",
    "text": "We first apply the SVD to an affine functions. The physical space of the dataset is \\(\\mathbb{R}^2\\) but the data set is in fact 1D. The SVD is used to find a suitable 1D linear subspace of \\(\\mathbb{R}^2\\) in which the data can be projected without loosing any information.\n\n\nWe start by defining the function and by computing the SVD of all the 2D snapshots.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nn_sample = 100                                                                              # Choose number of samples\nx = np.linspace(0,10,n_sample)                                                              # create vector x\n\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = a*x+b                                                                                   # Affine function\nX = np.stack((x,y))                                                                         # Create numpy array\n\n# Noise\nn_coef = 1                                                                                  # Noise magnitude\nX += + n_coef*np.random.rand(*X.shape)                                                      # Noisy data\n\n# SVD\nU,S,V = np.linalg.svd(X)                                                                   # SV Decomposition\n\n\n\n\n\nTo visualise the underlying space found in the data by the SVD, we plot the principal directions \\(\\nu_1\\) and \\(\\nu_2\\) onto the initial dataset.\n\n\nCode\ncolors_1D = plt.cm.viridis(np.linspace(0, 1, len(X[0,:])))                                        # Generate a color gradient\n\n# plots\n# Origin point\norigin = np.array([[0, 0], [0, 0]]) \nU_scaled = -5*U\nplt.figure(figsize=(6, 6))\n# plt.quiver(*origin, U_scaled[:, 0], U_scaled[:, 1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])\nplt.quiver(*origin, U_scaled[0, 0], U_scaled[0, 1], angles='xy', scale_units='xy', scale=1, color='#01426A', label=r'$\\nu_1$')\nplt.quiver(*origin, U_scaled[1, 0], U_scaled[1, 1], angles='xy', scale_units='xy', scale=1, color='#CE0037', label=r'$\\nu_2$')\nplt.legend()\nplt.gca().set_aspect('equal')\nplt.ylim(-4, 15)\nplt.xlim(-4, 15)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\n# plt.plot(X[0,:],X[1,:],'+')\n# plt.scatter(X[0,:],X[1,:], color=colors_1D, s=100)           \nplt.scatter(X[0,:],X[1,:], facecolors='none', edgecolors=colors_1D, s=100)             \n\nplt.title(\"Principal component base and data points\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nWe then project the data onto the latent space \\(\\left(\\nu_1,\\nu_2\\right)\\).\n\n\nCode\n# in the latent space\n\nX_tilde = np.transpose(U)@X\n# plt.plot(X_tilde[0,:],X_tilde[1,:],\"-o\")\nplt.scatter(X_tilde[0,:],X_tilde[1,:], color=colors_1D, s=100)           \n\nplt.gca().set_aspect('equal', adjustable='box')\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\nWe can see that the initial 2D data in fact lies in a 1D space and that the first principal direction \\(\\nu_1\\) is sufficient to describe the full data set."
  },
  {
    "objectID": "kPCA_v_SVD.html#application-to-a-noisy-affine-function-1",
    "href": "kPCA_v_SVD.html#application-to-a-noisy-affine-function-1",
    "title": "Non-linear manifold learning",
    "section": "Application to a noisy affine function",
    "text": "Application to a noisy affine function\nWe now apply the SVD to an affine function. The physical space of the dataset is still \\(\\mathbb{R}3\\) but the data set is in fact 2D. The kPCA is used to find a suitable 2D linear subspace similarly to what the SVD would give.\n\nFunction definition\n\n\nCode\nx = np.linspace(0,4*np.pi,1500)\n# Affine parameters\na = 1.5                                                                                     # Slope\nb=0                                                                                         # y-axis origin\n\n# Affine function\ny = a*x+b \nn_coef = 1                                                                      # Noise level\nz = np.random.randn(*x.shape)        \nX = np.stack((x,y,z))                                                           # Create 3D dataset\nX += + n_coef*np.random.rand(*X.shape)                                          # Add noise\n\n\nNote: The PCA conventions are transposed compared to SVD’s conventions\n\n\nCode\nY = np.transpose(X)\n\n\n\n\nkPCA parameters\nWe define the kPCA paramerets, namely\n\nThe number of components\n\\(\\sigma\\), the width of the kernel, controlling how much distant point are considered similar\n\nA smaller \\(\\sigma\\) emphasises local structures, while a larger σσ captures more global patterns.\n\n\n\n\nCode\nn_components = 2                                                                 # Number of component of the kpca\nsigma =100                                                                       # sigma for Gaussian kernel\n\n\n\n\nCompute the (Gaussian) kernel\nWe now compute the kernel matrix based on the distance matrix using a Gaussian kernel\n\\[K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\\]\n\n\nCode\nd_2 = np.sum(Y**2, axis=1).reshape(-1, 1) + np.sum(Y**2, axis=1) - 2 * np.dot(Y, Y.T)\nK = np.exp(-d_2 / (2 * sigma**2))\n\n\n\n\nCenter the kernel matrix\nWe then center the kernel matrix as the PCA expects 0-mean data\n\\[\nK_{\\text{centered}} = K - \\frac{1}{n} \\mathbf{1} K - \\frac{1}{n} K \\mathbf{1} + \\frac{1}{n^2} \\mathbf{1} K \\mathbf{1}\n\\]\n\n\nCode\nn = K.shape[0]\none_n = np.ones((n, n)) / n\nK_centered = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n\n\nWe perform an SVD of the centered kernel to find the principal directions of the high dimensional space,\n\n\nCode\nU, S, V = np.linalg.svd(K_centered)\n\n\nthat we then truncates. we then project the initial dataset onto the principal directions in the latent space\n\n\nCode\nU_k = U[:, :n_components]  # first eigenvectors\nS_k = np.diag(S[:n_components])  # first singular values\n# Project the centered kernel matrix onto the first eigenvectors\nY_kpca = U_k @ S_k  \n\n\n\n\nPlots\nIn the physical space (the plot is interactive, feel free to explore the 3D dataset !)\n\n\nCode\ncolors = plt.cm.viridis(np.linspace(0, 1, len(X[0,:])))                                        # Generate a color gradient\n\nimport plotly.graph_objects as go\n\n\n# colors_plotly = colors[:, :3]\ncolors_plotly = np.linspace(0, 1, X.shape[1])\n\nx_points = X[0, :]\ny_points = X[1, :]\nz_points = X[2, :]\n\n# Create the 3D scatter plot\nfig = go.Figure(data=[go.Scatter3d(\n    x=x_points,\n    y=y_points,\n    z=z_points,\n    mode='markers',\n    marker=dict(\n        size=5,                                                                         # Size of the markers\n        color=colors_plotly,                                                            # Color of the markers\n        colorscale='viridis',                                                           # Color scale\n        showscale=True,                                                                 # Show color scale\n    )\n)])\n\nfig.update_layout(\n    scene=dict(\n        xaxis_title='x',\n        yaxis_title='y',\n        zaxis_title='z',\n    ),\n    title='Physical Space'\n)\nfig.show()\n\n\n                                                \n\n\nIn the latent space\n\n\nCode\nplt.scatter(Y_kpca[:,0], Y_kpca[:,1], color=colors, s=100)                                     # s=100 for larger points\nplt.xlabel(r\"$\\nu_1$\")\nplt.ylabel(r\"$\\nu_2$\")\nplt.title('latent space')\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that for linear data, the kPCA behaves similarly to the SVD."
  }
]